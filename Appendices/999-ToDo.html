

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>To Do &#8212; Managing AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Appendices/999-ToDo';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="prev" title="Bibliography" href="900-Bibliography.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../000-Root.html">
  
  
  
  
  
    <p class="title logo__title">Managing AI</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../000-Root.html">
                    Managing AI
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Product, Project, and Program</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ProductProjectProgram/010-Introduction.html">Product, Project, and Program</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../MachineLearning/010-Introduction.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MachineLearning/600-NeuralNetworks.html">Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Data/010-Introduction.html">Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Natural Language</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../NaturalLanguage/011-Linguistics.html">Linguistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NaturalLanguage/013-Corpora.html">Corpora</a></li>
<li class="toctree-l1"><a class="reference internal" href="../NaturalLanguage/500-Transformers.html">Transformers</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Technology</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Technology/070-Programming_Languages.html">Programming Languages</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Culture, People, and Organizations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../CulturePeopleOrganizations/010-Introduction.html">Culture, People, and Organizations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="190-Math_Typesetting.html">Appendix – Math Typesetting</a></li>
<li class="toctree-l1"><a class="reference internal" href="191-Charts.html">Charts Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="900-Bibliography.html">Bibliography</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">To Do</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/Appendices/999-ToDo.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>

</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>To Do</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="to-do">
<h1>To Do<a class="headerlink" href="#to-do" title="Permalink to this heading">#</a></h1>
<p>fix glue in ChartsDemo
citation / bibliography
open in colab button</p>
<p>load corpora (Federalist Papers, Shakespeare, …)</p>
<p>collection of publically available information and personal opinions
copyright notice</p>
<p>question answering
pos tagging
testing and data quality monitoring
Scaling Laws for Neural Language Models
Mark Liberman’s “golden age of nlp” talk
RACI Charts</p>
<p>change data capture
event sourcing</p>
<p>knowledge graphs
transformer architecture
encoder/decoder
self attention
hyperparameter optimization
technology landscape
cloud vendors
data
tools
notebooks</p>
<p>project management
budget
hiring
people management
triangulation and corroboration
proof
tradeoffs</p>
<ul class="simple">
<li><p>Costs: labor, cloud, hardware, software, data, …
differentiability
conferences
demo
product management
ongoing learning
education
LLM
GPUs
edge computing
model quality evaluation
document similarity
statistical fallacies
annotation teams
active learning
Snorkel
transfer learning
latency measurement
provenance
explainable
attribution
model updating Ala HAI’s Helm
edit models.  (eg trained on github)
prompt engineering
NER
document segmentation
history: cyc, Eliza, racter
bottleneck problem</p></li>
</ul>
<p>team topologies</p>
<p>glove
bert
bloom
GPT
xaviers catalog</p>
<p>legal bert</p>
<p>rlhf</p>
<p>repeatability
feature stores
experiment tracking
sentence splitting</p>
<p>culture</p>
<p>book.adoc content
document segmentation
document similarity
summarization
remainder of nlp google doc
no model is perfect, product context matters
start with data science and design first with 6 month lead time
PII
search
TFIDF, BM25
List metrics
when to go beyond OSS/OTS software?</p>
<p>“Attention is all you need” paper</p>
<p>Prompt Chaining
Eg: LangChain</p>
<p>Transfer Learning</p>
<p>https://www.linkedin.com/posts/juliaelzini_bert-gpt-transformer-activity-7046045851307167744-eo_E?utm_source=share&amp;utm_medium=member_android</p>
<p>https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html</p>
<p>https://atcold.github.io/pytorch-Deep-Learning/</p>
<p>https://deeppavlov.ai/about-us</p>
<p>LangChain
https://github.com/hwchase17/langchain</p>
<p>“Transformer Models: An Introduction and Catalog”
https://arxiv.org/abs/2302.07730
Author: Xavier Amatrian
Abstract: In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.</p>
<p>Semi-Supervised Learning
Wikipedia: https://en.wikipedia.org/wiki/Semi-Supervised_Learning#
Semi-supervised learning, otherwise termed weak supervision, is a branch of machine learning where noisy, limited, or imprecise sources are used to provide supervision signal for labeling large amounts of training data in a supervised learning setting.[1] This approach alleviates the burden of obtaining hand-labeled data sets, which can be costly or impractical. Instead, inexpensive weak labels are employed with the understanding that they are imperfect, but can nonetheless be used to create a strong predictive model.[2][3][4] Semi-supervised learning can therefore be seen as a reasonable middleground between supervised and unsupervised machine learning approaches.</p>
<p>Transformer
Wikipedia: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)
A transformer is a deep learning model that adopts the mechanism of self-attention, differentially weighting the significance of each part of the input data. It is used primarily in the fields of natural language processing (NLP)[1] and computer vision (CV).[2]
Like recurrent neural networks (RNNs), transformers are designed to process sequential input data, such as natural language, with applications towards tasks such as translation and text summarization. However, unlike RNNs, transformers process the entire input all at once. The attention mechanism provides context for any position in the input sequence. For example, if the input data is a natural language sentence, the transformer does not have to process one word at a time. This allows for more parallelization than RNNs and therefore reduces training times.[1]
Transformers were introduced in 2017 by a team at Google Brain[1] and are increasingly the model of choice for NLP problems,[3] replacing RNN models such as long short-term memory (LSTM). The additional training parallelization allows training on larger datasets. This led to the development of pretrained systems such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformer), which were trained with large language datasets, such as the Wikipedia Corpus and Common Crawl, and can be fine-tuned for specific tasks.[4][5]</p>
<p>Generative Adversarial Network (GAN)
Wikipedia: https://en.wikipedia.org/wiki/Generative_adversarial_network
A generative adversarial network (GAN) is a class of machine learning frameworks designed by Ian Goodfellow and his colleagues in June 2014.[1] Two neural networks contest with each other in the form of a zero-sum game, where one agent’s gain is another agent’s loss.
Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of generative model for unsupervised learning, GANs have also proved useful for semi-supervised learning,[2] fully supervised learning,[3] and reinforcement learning.[4]
The core idea of a GAN is based on the “indirect” training through the discriminator, another neural network that can tell how “realistic” the input seems, which itself is also being updated dynamically.[5] This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.
GANs are similar to mimicry in evolutionary biology, with an evolutionary arms race between both networks.</p>
<p>GPT (not ChatGPT)
GAN
BERT
Large Language Model (LLM)
Reinforcement Learning
Active Learning
Self-Supervision</p>
<p>Daniel Lewis found this Meta paper re PEER: https://arxiv.org/pdf/2208.11663.pdf?utm_source=pocket_saves</p>
<p>Microsoft Promptist
https://arxiv.org/abs/2212.06713
https://huggingface.co/spaces/microsoft/Promptist
https://github.com/microsoft/LMOps</p>
<p>prompt tuning
prompt engineering</p>
<p>Prompt Tuning info from Ben Lorica</p>
<p>Llama from Meta https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/</p>
<p>Dolly</p>
<p>https://writings.stephenwolfram.com/2023/02/what-is-chatgpt-doing-and-why-does-it-work/</p>
<p>https://aibusiness.com/meta/ai-luminary-yann-lecunn-sets-us-straight-on-generative-ai</p>
<p>model cards
learning to rank
One-Shot, Few-Shot
Transfer Learning
Convolutional
LSTM
Support Vector Machine (SVM)
MLM
Gradient Boosted Trees</p>
<p>MEND to edit LLMs https://hai.stanford.edu/news/how-do-we-fix-and-update-large-language-models</p>
<p>spacy course https://www.datacamp.com/courses/advanced-nlp-with-spacy
https://course.spacy.io/en/</p>
<p>Commercial and Open Source Software</p>
<p>AWS Ground Truth
AWS Human in the Loop
Snorkel
SparkNLP
https://nlp.johnsnowlabs.com/docs/en/install
OpenAI
Google Bard commercial fuck-up. What could they have done?
BERT
RoBERTA
Lambda
Spacey
Fixie</p>
<p>Argilla
Website: https://docs.argilla.io/en/latest/index.html
Quickstart: https://docs.argilla.io/en/latest/getting_started/quickstart.html
Argilla Client: a powerful Python library for reading and writing data into Argilla, using all the libraries you love (transformers, spaCy, datasets, and any other).
Argilla Server and UI: the API and UI for data annotation and curation.</p>
<p>Create New Hugging Face space
https://huggingface.co/spaces/pingel/ArgillaTest?logs=build
github</p>
<p>“Why Vector Search Now?”
Doug Turnbull
https://softwaredoug.com/blog/2023/02/13/why-vector-search.html</p>
<p>“Domain-Specific Intelligent Bots”
Arun Shankar
https://medium.com/&#64;shankar.arunp/chatgpt-decoded-an-expert-guide-to-mastering-the-technology-and-building-domain-specific-3a95b42827bb</p>
<p>Zero-Shot Learning
Wikipedia: https://en.wikipedia.org/wiki/Zero-shot_learning
Zero-shot learning (ZSL) is a problem setup in machine learning where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects.[1] For example, given a set of images of animals to be classified, along with auxiliary textual descriptions of what animals look like, an artificial intelligence model which has been trained to recognize horses, but has never been given a zebra, can still recognize a zebra when it also knows that zebras look like striped horses. This problem is widely studied in computer vision, natural language processing, and machine perception.[2]</p>
<p>GPT-3
Wikipedia: https://en.wikipedia.org/wiki/GPT-3
Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that uses deep learning to produce human-like text. Given an initial text as prompt, it will produce text that continues the prompt.
The architecture is a decoder-only transformer network with a 2048-token-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model was trained using generative pre-training; it is trained to predict what the next token is based on previous tokens. The model demonstrated strong zero-shot and few-shot learning on many tasks.[2] The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of “generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.” This eliminated the need for human supervision and for time-intensive hand-labeling.[2]</p>
<p>Yann LeCun “A Path Towards Autonomous Machine Intelligence”
https://openreview.net/pdf?id=BZ5a1r-kVsf</p>
<p>ChatGPT
Wikipedia: https://en.wikipedia.org/wiki/ChatGPT
ChatGPT (Chat Generative Pre-trained Transformer[2]) is a chatbot developed by OpenAI and launched in November 2022. It is built on top of OpenAI’s GPT-3 family of large language models and has been fine-tuned (an approach to transfer learning) using both supervised and reinforcement learning techniques.</p>
<p>Let’s Build ChatGPT: From Scratch, In Code, Spelled Out
Youtube: https://www.youtube.com/watch?v=kCc8FmEb1nY</p>
<p>LEGAL-BERT
https://arxiv.org/pdf/2010.02559.pdf</p>
<p>“What Learning Algorithm is In-Context Learning? Investigations with Linear Models“
arxiv: https://arxiv.org/pdf/2211.15661.pdf
Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples (x, f(x)) presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners’ late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at this https link.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./Appendices"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="900-Bibliography.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Bibliography</p>
      </div>
    </a>
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Adam Pingel
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>