

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Natural Langauge &#8212; Managing AI</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css" />
    <link rel="stylesheet" href="../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'NaturalLanguage/010-Introduction';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../000-Root.html">
  
  
  
  
  
    <p class="title logo__title">Managing AI</p>
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../000-Root.html">
                    Managing AI
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Product, Project, and Program</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../ProductProjectProgram/010-Introduction.html">Product, Project, and Program</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Machine Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../MachineLearning/010-Introduction.html">Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../MachineLearning/600-NeuralNetworks.html">Neural Networks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Data</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Data/010-Introduction.html">Data</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Natural Language</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="011-Linguistics.html">Linguistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="013-Corpora.html">Corpora</a></li>
<li class="toctree-l1"><a class="reference internal" href="500-Transformers.html">Transformers</a></li>
<li class="toctree-l1"><a class="reference internal" href="590-ChatGPT.html">ChatGPT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Technology</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Technology/010-Introduction.html">Technology</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Technology/070-Programming_Languages.html">Programming Languages</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Culture, People, and Organizations</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../CulturePeopleOrganizations/010-Introduction.html">Culture, People, and Organizations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Appendices</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Appendices/190-Math_Typesetting.html">Appendix – Math Typesetting</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendices/191-Charts.html">Charts Demo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendices/900-Bibliography.html">Bibliography</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Appendices/999-ToDo.html">To Do</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/NaturalLanguage/010-Introduction.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Natural Langauge</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Natural Langauge</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topics">Topics</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#in-context-learning">In-Context Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-ecosystem">Transformer Ecosystem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#legal-bert">Legal BERT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-3">GPT-3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-learning">Zero-Shot Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#editing-llms">Editing LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collaboration-models">Collaboration Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#peer-a-collaborative-language-model">PEER: A Collaborative Language Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-chaining">Prompt Chaining</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section id="natural-langauge">
<h1>Natural Langauge<a class="headerlink" href="#natural-langauge" title="Permalink to this heading">#</a></h1>
<section id="topics">
<h2>Topics<a class="headerlink" href="#topics" title="Permalink to this heading">#</a></h2>
<p>Chapters to be written:</p>
<ul class="simple">
<li><p>tokenization</p></li>
<li><p>sentence splitting</p></li>
<li><p>pos tagging</p></li>
<li><p>coreference resolution</p></li>
<li><p>baive nayes (NLP applications)</p></li>
<li><p>bag of words</p></li>
<li><p>word2vec</p></li>
<li><p>sentiment analysis</p></li>
<li><p>NER</p></li>
<li><p>document segmentation</p></li>
<li><p>document summarization</p></li>
<li><p>topic modeling</p></li>
<li><p>question answering</p></li>
<li><p>Scaling Laws for Neural Language Models</p></li>
<li><p>Mark Liberman’s “golden age of nlp” talk</p></li>
<li><p>transformer architecture</p></li>
<li><p>encoder/decoder</p></li>
<li><p>self attention</p></li>
<li><p>hyperparameter optimization (for NLP specifically)</p></li>
<li><p>document similarity</p></li>
<li><p>statistical fallacies</p></li>
<li><p>annotation teams</p></li>
<li><p>active learning</p></li>
<li><p>Snorkel</p></li>
<li><p>transfer learning</p></li>
<li><p>explainability</p></li>
<li><p>history: Eliza, Racter, cyc</p></li>
<li><p>“Attention is all you need” paper</p></li>
<li><p>glove</p></li>
<li><p>bert</p></li>
<li><p>bloom</p></li>
<li><p>GPT</p></li>
<li><p>Xavier Amatrian’s catalog</p></li>
<li><p>RLHF</p></li>
<li><p>edit models.  (eg trained on github)</p></li>
<li><p>prompt engineering</p></li>
<li><p>Llama from Meta https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/</p></li>
<li><p>Alpaca</p></li>
<li><p>Dolly https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html</p></li>
<li><p>Cerebras</p></li>
<li><p>Microsoft Promptist</p>
<ul>
<li><p>https://arxiv.org/abs/2212.06713</p></li>
<li><p>https://huggingface.co/spaces/microsoft/Promptist</p></li>
<li><p>https://github.com/microsoft/LMOps</p></li>
</ul>
</li>
<li><p>Prompt Tuning info from Ben Lorica</p></li>
</ul>
</section>
</section>
<section id="in-context-learning">
<h1>In-Context Learning<a class="headerlink" href="#in-context-learning" title="Permalink to this heading">#</a></h1>
<p>“What Learning Algorithm is In-Context Learning? Investigations with Linear Models“
arxiv: https://arxiv.org/pdf/2211.15661.pdf</p>
<blockquote>
<div><p>Neural sequence models, especially transformers, exhibit a remarkable capacity for in-context learning. They can construct new predictors from sequences of labeled examples (x, f(x)) presented in the input without further parameter updates. We investigate the hypothesis that transformer-based in-context learners implement standard learning algorithms implicitly, by encoding smaller models in their activations, and updating these implicit models as new examples appear in the context. Using linear regression as a prototypical problem, we offer three sources of evidence for this hypothesis. First, we prove by construction that transformers can implement learning algorithms for linear models based on gradient descent and closed-form ridge regression. Second, we show that trained in-context learners closely match the predictors computed by gradient descent, ridge regression, and exact least-squares regression, transitioning between different predictors as transformer depth and dataset noise vary, and converging to Bayesian estimators for large widths and depths. Third, we present preliminary evidence that in-context learners share algorithmic features with these predictors: learners’ late layers non-linearly encode weight vectors and moment matrices. These results suggest that in-context learning is understandable in algorithmic terms, and that (at least in the linear case) learners may rediscover standard estimation algorithms. Code and reference implementations are released at this https link.</p>
</div></blockquote>
<section id="transformer-ecosystem">
<h2>Transformer Ecosystem<a class="headerlink" href="#transformer-ecosystem" title="Permalink to this heading">#</a></h2>
<p>“Transformer Models: An Introduction and Catalog”
https://arxiv.org/abs/2302.07730
Author: Xavier Amatrian</p>
<blockquote>
<div><p>Abstract: In the past few years we have seen the meteoric appearance of dozens of models of the Transformer family, all of which have funny, but not self-explanatory, names. The goal of this paper is to offer a somewhat comprehensive but simple catalog and classification of the most popular Transformer models. The paper also includes an introduction to the most important aspects and innovation in Transformer models.</p>
</div></blockquote>
</section>
<section id="legal-bert">
<h2>Legal BERT<a class="headerlink" href="#legal-bert" title="Permalink to this heading">#</a></h2>
<p>LEGAL-BERT https://arxiv.org/pdf/2010.02559.pdf</p>
</section>
<section id="gpt-3">
<h2>GPT-3<a class="headerlink" href="#gpt-3" title="Permalink to this heading">#</a></h2>
<p>Wikipedia: https://en.wikipedia.org/wiki/GPT-3</p>
<blockquote>
<div><p>Generative Pre-trained Transformer 3 (GPT-3) is an autoregressive language model released in 2020 that uses deep learning to produce human-like text. Given an initial text as prompt, it will produce text that continues the prompt.</p>
</div></blockquote>
<blockquote>
<div><p>The architecture is a decoder-only transformer network with a 2048-token-long context and then-unprecedented size of 175 billion parameters, requiring 800GB to store. The model was trained using generative pre-training; it is trained to predict what the next token is based on previous tokens. The model demonstrated strong zero-shot and few-shot learning on many tasks.[2] The authors described how language understanding performances in natural language processing (NLP) were improved in GPT-n through a process of “generative pre-training of a language model on a diverse corpus of unlabeled text, followed by discriminative fine-tuning on each specific task.” This eliminated the need for human supervision and for time-intensive hand-labeling.[2]</p>
</div></blockquote>
</section>
<section id="zero-shot-learning">
<h2>Zero-Shot Learning<a class="headerlink" href="#zero-shot-learning" title="Permalink to this heading">#</a></h2>
<p>Wikipedia: https://en.wikipedia.org/wiki/Zero-shot_learning</p>
<blockquote>
<div><p>Zero-shot learning (ZSL) is a problem setup in machine learning where, at test time, a learner observes samples from classes which were not observed during training, and needs to predict the class that they belong to. Zero-shot methods generally work by associating observed and non-observed classes through some form of auxiliary information, which encodes observable distinguishing properties of objects.[1] For example, given a set of images of animals to be classified, along with auxiliary textual descriptions of what animals look like, an artificial intelligence model which has been trained to recognize horses, but has never been given a zebra, can still recognize a zebra when it also knows that zebras look like striped horses. This problem is widely studied in computer vision, natural language processing, and machine perception.[2]</p>
</div></blockquote>
</section>
<section id="editing-llms">
<h2>Editing LLMs<a class="headerlink" href="#editing-llms" title="Permalink to this heading">#</a></h2>
<p>MEND https://hai.stanford.edu/news/how-do-we-fix-and-update-large-language-models</p>
</section>
<section id="collaboration-models">
<h2>Collaboration Models<a class="headerlink" href="#collaboration-models" title="Permalink to this heading">#</a></h2>
<section id="peer-a-collaborative-language-model">
<h3>PEER: A Collaborative Language Model<a class="headerlink" href="#peer-a-collaborative-language-model" title="Permalink to this heading">#</a></h3>
<p>From Meta</p>
<p>https://arxiv.org/pdf/2208.11663.pdf?utm_source=pocket_saves</p>
<blockquote>
<div><p>Textual content is often the output of a collaborative writing process: We start with an
initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today’s language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of selftraining techniques for increasing the quality, amount and diversity of training data. This unlocks PEER’s full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.</p>
</div></blockquote>
</section>
</section>
<section id="prompt-chaining">
<h2>Prompt Chaining<a class="headerlink" href="#prompt-chaining" title="Permalink to this heading">#</a></h2>
<p>LangChain https://github.com/hwchase17/langchain</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./NaturalLanguage"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Natural Langauge</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#topics">Topics</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#in-context-learning">In-Context Learning</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer-ecosystem">Transformer Ecosystem</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#legal-bert">Legal BERT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gpt-3">GPT-3</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#zero-shot-learning">Zero-Shot Learning</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#editing-llms">Editing LLMs</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#collaboration-models">Collaboration Models</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#peer-a-collaborative-language-model">PEER: A Collaborative Language Model</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prompt-chaining">Prompt Chaining</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Adam Pingel
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>